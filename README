------------------------------------------------------------------
SCRAPING TEXT

We use data stored at http://www.presidency.ucsb.edu/index.php

scrape.py will download all the files listed on a webpage with
a form like:

http://www.presidency.ucsb.edu/2016_election_speeches.php?candidate=70&campaign=2016CLINTON&doctype=5000

the shell script runScraper.sh contains examples for executing the
python scrip. 

scrape.py will save one file per speech with the following path:

data/rawtext/[electionyear]/[D/R]/[speakerinitial]_[filenum]

a list of all the files in a particular directory is save in files.list

NOTES:

HC_0 - HC_106 were removed because they were dated in 2007 and 2008
long before the 2016 campaign.


------------------------------------------------------------------
PROCESSING TEXT

There are a set of python files titled getVocab*.py that can be
used to create BoW files in the format we want to use. These
require an input file list (.list) that lists the full paths
of all input files (note: this path must include /D/ or /R/ to denote
party affiliation)

For now, we can split the output file outfile.dat into 2/3 train
data and 1/3 test data using splitData.py noting that there is no
randomization involved in this process

------------------------------------------------------------------
ANALYZING TEXT

Right now, we have one analyzer: NaiveBayes.py (thanks Garrett!).
When run on the full corpus from the last three election cycles it
has a classification error of ~10%